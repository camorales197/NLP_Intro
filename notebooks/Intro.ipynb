{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Overview\n",
    "\n",
    "Welcome to this introductory notebook on Natural Language Processing (NLP). This notebook aims to provide an overview of NLP, its importance, and key techniques used in the field, along with code examples using Python libraries.\n",
    "\n",
    "## What is Natural Language Processing?\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of data science and artificial intelligence focused on enabling computers to understand, interpret, and generate human language in a valuable way. By utilizing NLP techniques, we can process large amounts of text data to perform tasks such as:\n",
    "\n",
    "- **Automatic Summarization**\n",
    "- **Machine Translation**\n",
    "- **Named Entity Recognition**\n",
    "- **Sentiment Analysis**\n",
    "- **Speech Recognition**\n",
    "- **Topic Segmentation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Why is Natural Language Processing Important?\n",
    "\n",
    "- **Facilitates Communication**: Enables seamless interaction between humans and computers, powering chatbots, virtual assistants, and translation systems.\n",
    "- **Extracts Meaningful Information**: Helps in deriving insights from unstructured text data like social media posts, reviews, and articles.\n",
    "- **Automates Tasks**: Automates language-related tasks such as document classification, report generation, and question answering.\n",
    "- **Personalizes Experiences**: Enhances user experiences through personalized recommendations and content filtering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Text Preprocessing\n",
    "\n",
    "Text data is often unstructured and noisy, requiring preprocessing to make it suitable for analysis. Text preprocessing involves:\n",
    "\n",
    "1. **Noise Removal**\n",
    "2. **Lexicon Normalization**\n",
    "3. **Object Standardization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Noise Removal\n",
    "\n",
    "Noise refers to irrelevant information in text, such as stop words, URLs, hashtags, mentions, and punctuations. Removing noise helps in focusing on the meaningful parts of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out this link   \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def remove_noise(text):\n",
    "    \"\"\"Remove noise from text by eliminating URLs, mentions, hashtags, punctuations, and digits.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text with noise removed.\n",
    "    \"\"\"\n",
    "    # Remove URLs using regex pattern that matches 'http' followed by any non-whitespace characters\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions (e.g., @user) and hashtags (e.g., #topic) using regex pattern that matches '@' or '#' followed by word characters\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuations by translating each punctuation character to None\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove digits using regex pattern that matches one or more digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Sample text containing a URL, hashtag, and mention\n",
    "sample_text = \"Check out this link: https://example.com #NLP @user123\"\n",
    "\n",
    "# Clean the sample text by removing noise\n",
    "clean_text = remove_noise(sample_text)\n",
    "\n",
    "# Print the cleaned text\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See examples with https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lexicon Normalization\n",
    "\n",
    "Lexicon normalization is a crucial preprocessing step in NLP that involves reducing words to their root or base form.\n",
    "This helps in minimizing the complexity of text data and ensures that different forms of a word are treated as a single item.\n",
    "\n",
    "- **Stemming**: This technique truncates words to their base or root form, often by removing suffixes. For example, \"running\" becomes \"run\", \"happily\" becomes \"happi\", and \"cats\" becomes \"cat\".\n",
    "  - **Use Cases**:\n",
    "    - **Search Engines**: Improves search results by matching different forms of a word to a single base form.\n",
    "    - **Text Classification**: Reduces the dimensionality of feature space, making models more efficient.\n",
    "    - **Information Retrieval**: Enhances the retrieval of documents by treating different word forms as equivalent.\n",
    "\n",
    "- **Lemmatization**: This technique converts words to their dictionary or base form, considering the context and part of speech. For example, \"better\" becomes \"good\", \"am\" becomes \"be\", and \"geese\" becomes \"goose\".\n",
    "  - **Use Cases**:\n",
    "    - **Machine Translation**: Ensures accurate translation by considering the context and part of speech.\n",
    "    - **Chatbots**: Improves understanding of user input by normalizing words to their base form.\n",
    "    - **Sentiment Analysis**: Enhances the accuracy of sentiment classification by treating different forms of a word as a single item.\n",
    "\n",
    "Both techniques are essential for text normalization, but they serve different purposes and have different use cases. Stemming is generally faster and more aggressive, which can lead to non-dictionary forms. Lemmatization, on the other hand, is more curate as it considers the context and part of speech, but it is computationally more intensive.\n",
    "\n",
    "Examples:\n",
    "- Original: \"The striped bats are hanging on their feet for best\"\n",
    "- Stemming: \"the stripe bat are hang on their feet for best\"\n",
    "- Lemmatization: \"the stripe bat be hang on their foot for good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n",
      "Lemmatization: ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'feet', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Stemming\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemming:\", stems)\n",
    "\n",
    "# Lemmatization\n",
    "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(\"Lemmatization:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Object Standardization\n",
    "\n",
    "Standardizing text involves converting slang, abbreviations, and colloquial terms to a standard form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot do this anymore. It's not fair!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "contractions_dict = {\"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\"}\n",
    "def standardize_text(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions_dict:\n",
    "            text = text.replace(word, contractions_dict[word.lower()])\n",
    "    return text\n",
    "\n",
    "sample_text = \"I can't do this anymore. It's not fair!\"\n",
    "standard_text = standardize_text(sample_text)\n",
    "print(standard_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Text to Features (Feature Engineering)\n",
    "\n",
    "Transforming text into numerical features is essential for machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Syntactic Parsing\n",
    "\n",
    "Analyzing the grammatical structure of sentences.\n",
    "\n",
    "- **Part-of-Speech (POS) Tagging**: Assigning word types to each word in a sentence, such as nouns, verbs, adjectives, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carlosmorales/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"I am learning Natural Language Processing\"\n",
    "words = word_tokenize(text)\n",
    "pos_tags = pos_tag(words)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained POS Tags: [('I', 'PRP', 'Personal pronoun'), ('am', 'VBP', 'Verb, non-3rd person singular present'), ('learning', 'VBG', 'Verb, gerund or present participle'), ('Natural', 'NNP', 'Proper noun, singular'), ('Language', 'NNP', 'Proper noun, singular'), ('Processing', 'NNP', 'Proper noun, singular')]\n"
     ]
    }
   ],
   "source": [
    "def explain_pos_tags(pos_tags):\n",
    "    \"\"\"Provide explanations for POS tags.\"\"\"\n",
    "    explanations = {\n",
    "        'CC': 'Coordinating conjunction',\n",
    "        'CD': 'Cardinal number',\n",
    "        'DT': 'Determiner',\n",
    "        'EX': 'Existential there',\n",
    "        'FW': 'Foreign word',\n",
    "        'IN': 'Preposition or subordinating conjunction',\n",
    "        'JJ': 'Adjective',\n",
    "        'JJR': 'Adjective, comparative',\n",
    "        'JJS': 'Adjective, superlative',\n",
    "        'LS': 'List item marker',\n",
    "        'MD': 'Modal',\n",
    "        'NN': 'Noun, singular or mass',\n",
    "        'NNS': 'Noun, plural',\n",
    "        'NNP': 'Proper noun, singular',\n",
    "        'NNPS': 'Proper noun, plural',\n",
    "        'PDT': 'Predeterminer',\n",
    "        'POS': 'Possessive ending',\n",
    "        'PRP': 'Personal pronoun',\n",
    "        'PRP$': 'Possessive pronoun',\n",
    "        'RB': 'Adverb',\n",
    "        'RBR': 'Adverb, comparative',\n",
    "        'RBS': 'Adverb, superlative',\n",
    "        'RP': 'Particle',\n",
    "        'SYM': 'Symbol',\n",
    "        'TO': 'to',\n",
    "        'UH': 'Interjection',\n",
    "        'VB': 'Verb, base form',\n",
    "        'VBD': 'Verb, past tense',\n",
    "        'VBG': 'Verb, gerund or present participle',\n",
    "        'VBN': 'Verb, past participle',\n",
    "        'VBP': 'Verb, non-3rd person singular present',\n",
    "        'VBZ': 'Verb, 3rd person singular present',\n",
    "        'WDT': 'Wh-determiner',\n",
    "        'WP': 'Wh-pronoun',\n",
    "        'WP$': 'Possessive wh-pronoun',\n",
    "        'WRB': 'Wh-adverb'\n",
    "    }\n",
    "    return [(word, tag, explanations.get(tag, \"Unknown\")) for word, tag in pos_tags]\n",
    "\n",
    "\n",
    "\n",
    "# Explain POS tags\n",
    "explained_pos_tags = explain_pos_tags(pos_tags)\n",
    "\n",
    "print(\"Explained POS Tags:\", explained_pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.2 Entity Extraction\n",
    "\n",
    "Identifying important entities like names, locations, organizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.2.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "# Download the 'en_core_web_sm' model if not already present\n",
    "download('en_core_web_sm')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Statistical Features\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)**: This statistical measure evaluates the importance of a word in a document relative to a collection of documents (corpus). It is calculated by multiplying two metrics:\n",
    "\n",
    "1. **Term Frequency (TF)**: The number of times a word appears in a document, normalized by the total number of words in that document.\n",
    "   Example: In the document \"the cat sat on the mat\", the term frequency of \"the\" is 2/6 = 0.33.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: The logarithm of the total number of documents divided by the number of documents containing the word. This helps to reduce the weight of commonly used words and increase the weight of rare words.\n",
    "   Example: If the word \"cat\" appears in 3 out of 10 documents, the IDF is log(10/3) ≈ 0.52.\n",
    "\n",
    "The TF-IDF score for a word is the product of its TF and IDF scores. This score reflects how important a word is to a specific document in the context of the entire corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Important NLP Tasks\n",
    "\n",
    "### 3.1 Text Classification\n",
    "\n",
    "Categorizing text into predefined classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "texts = ['I love this movie', 'I hate this movie', 'This movie is great', 'This movie is terrible']\n",
    "labels = ['positive', 'negative', 'positive', 'negative']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, labels)\n",
    "\n",
    "test_text = ['I really love this fantastic movie']\n",
    "test_X = vectorizer.transform(test_text)\n",
    "print(clf.predict(test_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3.2 Text Similarity\n",
    "\n",
    "Measuring similarity between texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.474330706497194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "text1 = \"Natural Language Processing is fun\"\n",
    "text2 = \"I find very fun learning about Natural Language Processing\"\n",
    "\n",
    "# Use TfidfVectorizer instead of CountVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "# Compute cosine similarity between the two vectors\n",
    "cos_sim = cosine_similarity(vectors[0:1], vectors[1:2])\n",
    "print(\"Cosine Similarity:\", cos_sim[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Text Similarity with Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/carlosmorales/Library/Caches/pypoetry/virtualenvs/nlp-intro-uCSESLWM-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new movie is awesome\n",
      " - The dog plays in the garden   : 0.0543\n",
      " - The new movie is so great     : 0.8939\n",
      " - A woman watches TV            : -0.0502\n",
      "The cat sits outside\n",
      " - The dog plays in the garden   : 0.2838\n",
      " - The new movie is so great     : -0.0029\n",
      " - A woman watches TV            : 0.1310\n",
      "A man is playing guitar\n",
      " - The dog plays in the garden   : 0.2277\n",
      " - The new movie is so great     : -0.0136\n",
      " - A woman watches TV            : -0.0327\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "    \"The new movie is awesome\",\n",
    "    \"The cat sits outside\",\n",
    "    \"A man is playing guitar\",\n",
    "]\n",
    "\n",
    "sentences2 = [\n",
    "    \"The dog plays in the garden\",\n",
    "    \"The new movie is so great\",\n",
    "    \"A woman watches TV\",\n",
    "]\n",
    "\n",
    "# Compute embeddings for both lists\n",
    "embeddings1 = model.encode(sentences1)\n",
    "embeddings2 = model.encode(sentences2)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = model.similarity(embeddings1, embeddings2)\n",
    "\n",
    "# Output the pairs with their score\n",
    "for idx_i, sentence1 in enumerate(sentences1):\n",
    "    print(sentence1)\n",
    "    for idx_j, sentence2 in enumerate(sentences2):\n",
    "        print(f\" - {sentence2: <30}: {similarities[idx_i][idx_j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- NLTK Documentation: [http://www.nltk.org/](http://www.nltk.org/)\n",
    "- spaCy Documentation: [https://spacy.io/](https://spacy.io/)\n",
    "- Scikit-learn Documentation: [https://scikit-learn.org/](https://scikit-learn.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-intro-uCSESLWM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
